"""
Chat Analyzer - Ph√¢n t√≠ch batch c√°c tin nh·∫Øn t·ª´ MySQL events table
T·ªëi ∆∞u token b·∫±ng c√°ch g·ªôp nhi·ªÅu tin nh·∫Øn v√†o 1 l·∫ßn g·ªçi LLM
"""

import json
import time
import os
from typing import List, Dict, Any, Optional
from datetime import datetime
import mysql.connector
from mysql.connector import Error
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.messages import HumanMessage
from tenacity import (
    retry,
    stop_after_attempt,
    wait_exponential,
    retry_if_exception_type
)

# ========================================
# üîß C·∫§U H√åNH
# ========================================

DB_CONFIG = {
    'host': 'localhost',
    'user': 'root',
    'password': '101204',
    'database': 'agent_ai_db',
    'charset': 'utf8mb4'
}

# API Key - Thay b·∫±ng key c·ªßa b·∫°n ho·∫∑c d√πng bi·∫øn m√¥i tr∆∞·ªùng
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY", "AIzaSyCzsBMZaslKg5xnuRjEm8L7-D2bgHRPZIk")

# C·∫•u h√¨nh batch processing
BATCH_SIZE = 10  # S·ªë tin nh·∫Øn x·ª≠ l√Ω c√πng l√∫c (tƒÉng/gi·∫£m t√πy quota)
DELAY_BETWEEN_BATCHES = 65  # Gi√¢y ch·ªù gi·ªØa c√°c batch (tr√°nh rate limit)

# ========================================
# üß† KH·ªûI T·∫†O LLM V·ªöI RETRY
# ========================================

# D√πng model c√≥ quota cao h∆°n
llm = ChatGoogleGenerativeAI(
    model="gemini-2.5-flash",  # Model n√†y c√≥ quota cao h∆°n gemini-2.0-flash-exp
    temperature=0.1,
    google_api_key=GOOGLE_API_KEY,
    max_retries=3,
    timeout=120
)

# ========================================
# üìã SYSTEM PROMPT CHO BATCH ANALYSIS
# ========================================

BATCH_ANALYSIS_PROMPT = """B·∫°n l√† chuy√™n gia ph√¢n t√≠ch chat e-commerce. 

Nhi·ªám v·ª•: Ph√¢n t√≠ch T·∫§T C·∫¢ c√°c tin nh·∫Øn d∆∞·ªõi ƒë√¢y v√† tr·∫£ v·ªÅ k·∫øt qu·∫£ d·∫°ng JSON array.

C√°c ch·ªß ƒë·ªÅ (topic):
- TRA_CUU_DON_HANG: H·ªèi v·ªÅ ƒë∆°n h√†ng
- HOI_PHI_SHIP: H·ªèi v·ªÅ ph√≠ ship
- HOI_CHINH_SACH: H·ªèi v·ªÅ ƒë·ªïi tr·∫£, b·∫£o h√†nh
- TIM_KIEM_SAN_PHAM: T√¨m s·∫£n ph·∫©m
- TU_VAN_SAN_PHAM: T∆∞ v·∫•n, so s√°nh s·∫£n ph·∫©m
- HOI_KHUYEN_MAI: H·ªèi v·ªÅ voucher, gi·∫£m gi√°
- KHIEU_NAI_SAN_PHAM: Khi·∫øu n·∫°i s·∫£n ph·∫©m
- KHIEU_NAI_GIAO_HANG: Khi·∫øu n·∫°i giao h√†ng
- LOI_HE_THONG: L·ªói h·ªá th·ªëng
- UNKNOWN: Kh√¥ng x√°c ƒë·ªãnh

C·∫£m x√∫c (sentiment):
POSITIVE, NEUTRAL, NEGATIVE, FRUSTRATED, CONFUSED, URGENT

√ù ƒë·ªãnh mua (purchase_intent):
HIGH, MEDIUM, LOW, NONE

Entities c·∫ßn tr√≠ch xu·∫•t:
- product_id: M√£ SP (p001, SP123...)
- sku_id: M√£ SKU
- order_code: M√£ ƒë∆°n h√†ng
- shop_id: M√£ shop
- brand_name: Th∆∞∆°ng hi·ªáu
- category_name: Danh m·ª•c
- voucher_code: M√£ gi·∫£m gi√°

DANH S√ÅCH TIN NH·∫ÆN C·∫¶N PH√ÇN T√çCH:
{messages}

QUAN TR·ªåNG: Tr·∫£ v·ªÅ JSON array v·ªõi format:
[
  {{
    "message_index": 0,
    "topic": "TIM_KIEM_SAN_PHAM",
    "sentiment": "POSITIVE",
    "purchase_intent": "HIGH",
    "entities": [
      {{"type": "category_name", "context": "gi√†y th·ªÉ thao"}}
    ]
  }},
  {{
    "message_index": 1,
    ...
  }}
]

CH·ªà TR·∫¢ V·ªÄ JSON ARRAY, KH√îNG GI·∫¢I TH√çCH TH√äM!
"""

# ========================================
# üóÑÔ∏è DATABASE FUNCTIONS
# ========================================

def get_db_connection():
    """K·∫øt n·ªëi MySQL"""
    try:
        connection = mysql.connector.connect(**DB_CONFIG)
        if connection.is_connected():
            print("‚úÖ K·∫øt n·ªëi MySQL th√†nh c√¥ng!")
            return connection
    except Error as e:
        print(f" L·ªói k·∫øt n·ªëi MySQL: {e}")
        return None

def parse_content_field(content_str: str) -> Optional[str]:
    """Parse c·ªôt content ƒë·ªÉ l·∫•y text message"""
    try:
        content = json.loads(content_str)
        if 'parts' in content and len(content['parts']) > 0:
            return content['parts'][0].get('text', '')
        return None
    except:
        return None

def fetch_unprocessed_messages(connection, limit: int = BATCH_SIZE) -> List[Dict[str, Any]]:
    """
    L·∫•y c√°c tin nh·∫Øn ch∆∞a x·ª≠ l√Ω t·ª´ DB
    - author = 'user'
    - custom_metadata IS NULL (ch∆∞a ph√¢n t√≠ch)
    """
    cursor = connection.cursor(dictionary=True)
    
    query = """
    SELECT id, content, timestamp
    FROM events
    WHERE author = 'user' 
      AND custom_metadata IS NULL
      AND content IS NOT NULL
    ORDER BY timestamp ASC
    LIMIT %s
    """
    
    cursor.execute(query, (limit,))
    results = cursor.fetchall()
    cursor.close()
    
    # Parse content
    messages = []
    for row in results:
        text = parse_content_field(row['content'])
        if text and text.strip():
            messages.append({
                'id': row['id'],
                'text': text.strip(),
                'timestamp': row['timestamp']
            })
    
    return messages

def update_custom_metadata_batch(connection, updates: List[Dict[str, Any]]):
    """
    C·∫≠p nh·∫≠t custom_metadata cho nhi·ªÅu records c√πng l√∫c
    updates: [{"id": "...", "metadata": {...}}, ...]
    """
    cursor = connection.cursor()
    
    # Prepare batch update
    for update in updates:
        metadata_json = json.dumps(update['metadata'], ensure_ascii=False)
        
        query = """
        UPDATE events
        SET custom_metadata = %s
        WHERE id = %s
        """
        
        cursor.execute(query, (metadata_json, update['id']))
    
    connection.commit()
    cursor.close()
    print(f"‚úÖ ƒê√£ c·∫≠p nh·∫≠t {len(updates)} records v√†o DB")

# ========================================
# ü§ñ LLM ANALYSIS WITH RETRY
# ========================================

@retry(
    stop=stop_after_attempt(3),
    wait=wait_exponential(multiplier=1, min=60, max=300),
    retry=retry_if_exception_type(Exception)
)
def analyze_batch_with_llm(messages: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    G·ªçi LLM ƒë·ªÉ ph√¢n t√≠ch batch messages
    C√≥ retry mechanism v·ªõi exponential backoff
    """
    # Chu·∫©n b·ªã input cho LLM
    messages_text = ""
    for idx, msg in enumerate(messages):
        messages_text += f"\n--- Tin nh·∫Øn {idx} ---\n{msg['text']}\n"
    
    prompt = BATCH_ANALYSIS_PROMPT.format(messages=messages_text)
    
    print(f"üîÑ ƒêang g·ªçi LLM ph√¢n t√≠ch {len(messages)} tin nh·∫Øn...")
    
    try:
        response = llm.invoke([HumanMessage(content=prompt)])
        
        # Parse JSON response
        response_text = response.content.strip()
        
        # Remove markdown code blocks if present
        if response_text.startswith("```json"):
            response_text = response_text[7:]
        if response_text.startswith("```"):
            response_text = response_text[3:]
        if response_text.endswith("```"):
            response_text = response_text[:-3]
        
        results = json.loads(response_text.strip())
        
        print(f"‚úÖ LLM tr·∫£ v·ªÅ {len(results)} k·∫øt qu·∫£")
        return results
        
    except Exception as e:
        print(f" L·ªói khi g·ªçi LLM: {e}")
        raise

# ========================================
# üîÑ MAIN PROCESSING LOGIC
# ========================================

def process_batch(connection, messages: List[Dict[str, Any]]):
    """X·ª≠ l√Ω m·ªôt batch messages"""
    if not messages:
        print("Ô∏è Kh√¥ng c√≥ tin nh·∫Øn ƒë·ªÉ x·ª≠ l√Ω")
        return
    
    print(f"\n{'='*60}")
    print(f"üì¶ X·ª≠ l√Ω batch: {len(messages)} tin nh·∫Øn")
    print(f"{'='*60}")
    
    try:
        # G·ªçi LLM ph√¢n t√≠ch batch
        analysis_results = analyze_batch_with_llm(messages)
        
        # Chu·∫©n b·ªã d·ªØ li·ªáu update
        updates = []
        for result in analysis_results:
            idx = result.get('message_index')
            if idx is None or idx >= len(messages):
                continue
            
            msg = messages[idx]
            
            # T·∫°o metadata theo y√™u c·∫ßu (b·ªè confidence, reasoning)
            # Ch·ªâ gi·ªØ type v√† context cho entities
            clean_entities = [
                {
                    "type": e.get("type"),
                    "context": e.get("context")
                }
                for e in result.get('entities', [])
            ]
            
            metadata = {
                "topic": result.get('topic', 'UNKNOWN'),
                "sentiment": result.get('sentiment', 'NEUTRAL'),
                "purchase_intent": result.get('purchase_intent', 'NONE'),
                "entities": clean_entities,
                "analyzed_at": datetime.now().isoformat()
            }
            
            updates.append({
                'id': msg['id'],
                'metadata': metadata
            })
            
            # Log k·∫øt qu·∫£
            print(f"\nüìù ID: {msg['id'][:8]}...")
            print(f"   Message: {msg['text'][:50]}...")
            print(f"   Topic: {metadata['topic']}")
            print(f"   Sentiment: {metadata['sentiment']}")
            print(f"   Intent: {metadata['purchase_intent']}")
            print(f"   Entities: {len(clean_entities)}")
        
        # Batch update v√†o DB
        if updates:
            update_custom_metadata_batch(connection, updates)
            print(f"\n‚úÖ Ho√†n th√†nh batch: {len(updates)}/{len(messages)} tin nh·∫Øn")
        
    except Exception as e:
        print(f"\n L·ªói x·ª≠ l√Ω batch: {e}")
        print("‚è≠Ô∏è B·ªè qua batch n√†y v√† ti·∫øp t·ª•c...")

def main():
    """Main function"""
    print("\n" + "="*60)
    print("üöÄ CHAT ANALYZER - BATCH PROCESSING")
    print("="*60)
    
    # K·∫øt n·ªëi DB
    connection = get_db_connection()
    if not connection:
        print(" Kh√¥ng th·ªÉ k·∫øt n·ªëi DB. Tho√°t!")
        return
    
    try:
        total_processed = 0
        batch_count = 0
        
        while True:
            # L·∫•y batch messages ch∆∞a x·ª≠ l√Ω
            messages = fetch_unprocessed_messages(connection, BATCH_SIZE)
            
            if not messages:
                print("\n‚úÖ ƒê√£ x·ª≠ l√Ω xong t·∫•t c·∫£ tin nh·∫Øn!")
                break
            
            batch_count += 1
            print(f"\n{'='*60}")
            print(f"üîÑ BATCH #{batch_count}: T√¨m th·∫•y {len(messages)} tin nh·∫Øn ch∆∞a x·ª≠ l√Ω")
            print(f"{'='*60}")
            
            # X·ª≠ l√Ω batch
            process_batch(connection, messages)
            total_processed += len(messages)
            
            # Ch·ªù gi·ªØa c√°c batch ƒë·ªÉ tr√°nh rate limit
            if len(messages) == BATCH_SIZE:  # C√≤n batch ti·∫øp theo
                print(f"\n‚è≥ Ch·ªù {DELAY_BETWEEN_BATCHES}s tr∆∞·ªõc batch ti·∫øp theo (tr√°nh rate limit)...")
                time.sleep(DELAY_BETWEEN_BATCHES)
            
        # T·ªïng k·∫øt
        print("\n" + "="*60)
        print(f"üéâ HO√ÄN TH√ÄNH!")
        print(f"üìä T·ªïng s·ªë tin nh·∫Øn ƒë√£ x·ª≠ l√Ω: {total_processed}")
        print(f"üì¶ T·ªïng s·ªë batch: {batch_count}")
        print("="*60 + "\n")
        
    except KeyboardInterrupt:
        print("\nÔ∏è Ng∆∞·ªùi d√πng d·ª´ng ch∆∞∆°ng tr√¨nh")
    except Exception as e:
        print(f"\n L·ªói kh√¥ng mong mu·ªën: {e}")
    finally:
        if connection and connection.is_connected():
            connection.close()
            print("üîå ƒê√£ ƒë√≥ng k·∫øt n·ªëi MySQL")

# ========================================
# üèÉ RUN SCRIPT
# ========================================

if __name__ == "__main__":
    main()